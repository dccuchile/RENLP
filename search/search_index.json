{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ReLeLA The REpresentation LEarning and Natural LAnguage Processing (ReLeLa) research group located within the Department of Computer Science (DCC) at the University of Chile focuses on the intersection of representation learning and natural language processing. Members Academic Staff Jorge P\u00e9rez Felipe Bravo-Marquez Claudio Guti\u00e9rrez B\u00e1rbara Poblete Alexandre Bergel Students Constanza Fierro Juan Andr\u00e9s Moreno V\u00edctor Caro Pablo Badilla Juglar D\u00edaz Sebasti\u00e1n Donoso Frank Zamora Jhonny Cerezo Vicente Oyanedel Jos\u00e9 Ca\u00f1ete Daniel Aguirre Cristian Tamblay Henry Rosales Juan-Pablo Silva Camilo Garrido Cristi\u00e1n Ahumada Mabel S\u00e1nchez Hern\u00e1n Sarmiento Rolando Kindelan Collaborators Jocelyn Durstan Mauricio Quezada Felipe Tobar","title":"Home"},{"location":"#relela","text":"The REpresentation LEarning and Natural LAnguage Processing (ReLeLa) research group located within the Department of Computer Science (DCC) at the University of Chile focuses on the intersection of representation learning and natural language processing.","title":"ReLeLA"},{"location":"#members","text":"","title":"Members"},{"location":"#academic-staff","text":"Jorge P\u00e9rez Felipe Bravo-Marquez Claudio Guti\u00e9rrez B\u00e1rbara Poblete Alexandre Bergel","title":"Academic Staff"},{"location":"#students","text":"Constanza Fierro Juan Andr\u00e9s Moreno V\u00edctor Caro Pablo Badilla Juglar D\u00edaz Sebasti\u00e1n Donoso Frank Zamora Jhonny Cerezo Vicente Oyanedel Jos\u00e9 Ca\u00f1ete Daniel Aguirre Cristian Tamblay Henry Rosales Juan-Pablo Silva Camilo Garrido Cristi\u00e1n Ahumada Mabel S\u00e1nchez Hern\u00e1n Sarmiento Rolando Kindelan","title":"Students"},{"location":"#collaborators","text":"Jocelyn Durstan Mauricio Quezada Felipe Tobar","title":"Collaborators"},{"location":"courses/","text":"We teach the following courses: Deep Learning . Natural Language Processing Deep Learning Video Lectures (in Spanish) by Jorge P\u00e9rez Links to the video lectures of the first part of deep learning course: Peceptr\u00f3n, funciones de activaci\u00f3n, perceptr\u00f3n multicapa, redes feed forward (FF), expresividad de redes FF Funci\u00f3n de error/p\u00e9rdida y entrenamiento por descenso del gradiente Algoritmos de backpropagation Entropia Cruzada, Funcion de Perdida y Tensores Tensores y Backpropagation Pytorch y Entrenamiento de Redes Neuronales en la Pr\u00e1ctica Regularizaci\u00f3n Optimizaci\u00f3n I: Activaciones, Inicializaci\u00f3n de Par\u00e1metros, Batch Normalization y SGD con Momentum Optimizaci\u00f3n II: Nesterov, RMSProp, Adam y B\u00fasqueda de Hiperpar\u00e1metros","title":"Courses"},{"location":"courses/#deep-learning-video-lectures-in-spanish-by-jorge-perez","text":"Links to the video lectures of the first part of deep learning course: Peceptr\u00f3n, funciones de activaci\u00f3n, perceptr\u00f3n multicapa, redes feed forward (FF), expresividad de redes FF Funci\u00f3n de error/p\u00e9rdida y entrenamiento por descenso del gradiente Algoritmos de backpropagation Entropia Cruzada, Funcion de Perdida y Tensores Tensores y Backpropagation Pytorch y Entrenamiento de Redes Neuronales en la Pr\u00e1ctica Regularizaci\u00f3n Optimizaci\u00f3n I: Activaciones, Inicializaci\u00f3n de Par\u00e1metros, Batch Normalization y SGD con Momentum Optimizaci\u00f3n II: Nesterov, RMSProp, Adam y B\u00fasqueda de Hiperpar\u00e1metros","title":"Deep Learning Video Lectures (in Spanish) by Jorge P\u00e9rez"},{"location":"pointers/","text":"Pointers Faculty and Evolution of Language The faculty of language: what is it, who has it, and how did it evolve? The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky) http://downs-andrew.blogspot.com/2014/11/a-summary-of-chomsky.html?m=1 Chomsky and Piraha Language http://www.3ammagazine.com/3am/tom-wolfes-reflections-language/ https://www.lavocedinewyork.com/en/2016/10/04/chomsky-we-are-not-apes-our-language-faculty-is-innate/ Statistical (or neural) Language Models Noam Chomsky makes a strong argument about the limitations of statistical language models to model grammar (Syntatic Structures): It is natural to understand \"possible\" as meaning \"highly probable\" and to assume that the linguist's sharp distinction between grammatical and ungrammatical' is motivated by a feeling that since the `reality' of language is too complex to be described completely, he must content himself with a schematized version replacing \"zero probability, and all extremely low probabilities, by impossible, and all higher probabilities by possible.\" We see, however, that this idea is quite incorrect, and that a structural analysis cannot he understood as a schematic summary developed by sharpening the blurred edges in the full statistical picture. If we rank the sequences of a given length in order of statistical approximation to English, we will find both grammatical and ungrammatical sequences scattered throughout the list; there appears to be no particular relation between order of approximation and grammaticalness. Despite the undeniable interest and importance of semantic and statistical studies of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances. I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular https://archive.org/details/NoamChomskySyntcaticStructures/page/n31 There are strong counter-arguments to this: http://norvig.com/chomsky.html https://nlp.stanford.edu/manning/papers/probsyntax.pdf An empirical counter-argument would be the sentences generated by modern deep neural language models: https://openai.com/blog/better-language-models/#fn2 AI http://www.incompleteideas.net/IncIdeas/BitterLesson.html","title":"Pointers"},{"location":"pointers/#pointers","text":"","title":"Pointers"},{"location":"pointers/#faculty-and-evolution-of-language","text":"The faculty of language: what is it, who has it, and how did it evolve? The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky) http://downs-andrew.blogspot.com/2014/11/a-summary-of-chomsky.html?m=1","title":"Faculty and Evolution of Language"},{"location":"pointers/#chomsky-and-piraha-language","text":"http://www.3ammagazine.com/3am/tom-wolfes-reflections-language/ https://www.lavocedinewyork.com/en/2016/10/04/chomsky-we-are-not-apes-our-language-faculty-is-innate/","title":"Chomsky and Piraha Language"},{"location":"pointers/#statistical-or-neural-language-models","text":"Noam Chomsky makes a strong argument about the limitations of statistical language models to model grammar (Syntatic Structures): It is natural to understand \"possible\" as meaning \"highly probable\" and to assume that the linguist's sharp distinction between grammatical and ungrammatical' is motivated by a feeling that since the `reality' of language is too complex to be described completely, he must content himself with a schematized version replacing \"zero probability, and all extremely low probabilities, by impossible, and all higher probabilities by possible.\" We see, however, that this idea is quite incorrect, and that a structural analysis cannot he understood as a schematic summary developed by sharpening the blurred edges in the full statistical picture. If we rank the sequences of a given length in order of statistical approximation to English, we will find both grammatical and ungrammatical sequences scattered throughout the list; there appears to be no particular relation between order of approximation and grammaticalness. Despite the undeniable interest and importance of semantic and statistical studies of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances. I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular https://archive.org/details/NoamChomskySyntcaticStructures/page/n31 There are strong counter-arguments to this: http://norvig.com/chomsky.html https://nlp.stanford.edu/manning/papers/probsyntax.pdf An empirical counter-argument would be the sentences generated by modern deep neural language models: https://openai.com/blog/better-language-models/#fn2","title":"Statistical (or neural) Language Models"},{"location":"pointers/#ai","text":"http://www.incompleteideas.net/IncIdeas/BitterLesson.html","title":"AI"},{"location":"projects/","text":"Spanish Word Embeddings AffectiveTweets","title":"Projects"},{"location":"publications/","text":"2018 Jorge P\u00e9rez, Javier Marinkovi\u0107 and Pablo Barcel\u00f3, On the Turing Completeness of Modern Neural Network Architectures, ICLR 2019. ( pdf ) ( poster ) Aym\u00e9 Arango, Jorge P\u00e9rez, Barbara Poblete , Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation, SIGIR 2019. ( pdf ) Pablo Barcel\u00f3, Nelson Higuera, Jorge P\u00e9rez and Bernardo Subercaseaux, Expressiveness of Matrix and Tensor Query Languages in terms of ML Operators, DEEM @ SIGMOD 2019. ( pdf ) ( slides ) F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research 20(92): Pages 1\u22126, 2019. ( pdf ) S. Lang, F. Bravo-Marquez, C. Beckham, M. Hall, and E. Frank WekaDeeplearning4j: a Deep Learning Package for Weka based on DeepLearning4j , In Knowledge-Based Systems , Volume 178, 15 August 2019, Pages 48-50. DOI: 10.1016/j.knosys.2019.04.013 ( pdf ) A. Ansell, F. Bravo-Marquez, and B. Pfahringer An ELMo-inspired approach to SemDeep-5's Word-in-Context task . In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5) co-located with IJCAI 2019 in Macau, China. ( pdf ) D. Trye, A. S. Calude, F. Bravo-Marquez, and T. T Keegan M\u0101ori Loanwords: A Corpus of New Zealand English Tweets . In Proceedings of the 2019 ACL Student Research Workshop (SRW), Florence, Italy. ( pdf )","title":"Publications"},{"location":"publications/#2018","text":"Jorge P\u00e9rez, Javier Marinkovi\u0107 and Pablo Barcel\u00f3, On the Turing Completeness of Modern Neural Network Architectures, ICLR 2019. ( pdf ) ( poster ) Aym\u00e9 Arango, Jorge P\u00e9rez, Barbara Poblete , Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation, SIGIR 2019. ( pdf ) Pablo Barcel\u00f3, Nelson Higuera, Jorge P\u00e9rez and Bernardo Subercaseaux, Expressiveness of Matrix and Tensor Query Languages in terms of ML Operators, DEEM @ SIGMOD 2019. ( pdf ) ( slides ) F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research 20(92): Pages 1\u22126, 2019. ( pdf ) S. Lang, F. Bravo-Marquez, C. Beckham, M. Hall, and E. Frank WekaDeeplearning4j: a Deep Learning Package for Weka based on DeepLearning4j , In Knowledge-Based Systems , Volume 178, 15 August 2019, Pages 48-50. DOI: 10.1016/j.knosys.2019.04.013 ( pdf ) A. Ansell, F. Bravo-Marquez, and B. Pfahringer An ELMo-inspired approach to SemDeep-5's Word-in-Context task . In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5) co-located with IJCAI 2019 in Macau, China. ( pdf ) D. Trye, A. S. Calude, F. Bravo-Marquez, and T. T Keegan M\u0101ori Loanwords: A Corpus of New Zealand English Tweets . In Proceedings of the 2019 ACL Student Research Workshop (SRW), Florence, Italy. ( pdf )","title":"2018"},{"location":"reading/","text":"Reading List The following list contains topics we intend to discuss in our seminars: Language Models as Knowledge Bases Pointer Networks Bidirectional LSTM-CRF models for sequence tagging Sequence level training with recurrent neural networks : this paper introduces the idea of using reinforcement learning to optimize non-differentiable metrics such as BLEU. Neural Architecture Search with Reinforcement Learning","title":"Reading List"},{"location":"reading/#reading-list","text":"The following list contains topics we intend to discuss in our seminars: Language Models as Knowledge Bases Pointer Networks Bidirectional LSTM-CRF models for sequence tagging Sequence level training with recurrent neural networks : this paper introduces the idea of using reinforcement learning to optimize non-differentiable metrics such as BLEU. Neural Architecture Search with Reinforcement Learning","title":"Reading List"},{"location":"seminars/","text":"We hold weekly meetings on Wednesdays at 2PM at Auditorio Philippe Frajolet (303) Tercer Piso Edificio Poniente Beauchef 851. Our meeting's calendar Minutas (In Spanish) 25/09/2019: Wladmir Cardoso Brand\u00e3o present\u00f3 InferSent , una t\u00e9cnica para entrenar sentence embeddings usando datos de la Natural Language Inference task. 11/09/2019: Henry Rosales present\u00f3 su art\u00edculo publicado en EMNLP sobre Entity Linking. 04/09/2019: vimos la segunda parte del video de MultiTask Learning. Algunos conceptos interesantes: pointer networks (capas basadas en atenci\u00f3n para copiar partes del input), anti-curriculum training (aprender primero lo m\u00e1s d\u00edficil para evitar quedar en \u00f3ptimo local). 28/08/2019: Jorge di\u00f3 una clase sobre XLNet . Para llegar a XLNet hizo un repaso sobre Attention, Transformer y BERT . Cosas destacables sobre XLNet: relative positional encoding y permutation language models. Un blog post que trata de digerir esto aqu\u00ed . 21/08/2019: Daniel Aguirre present\u00f3 su charla de tesis I de mag\u00edster sobre Transformers para resolver tareas algor\u00edtmicas. 14/08/2019: Vimos este video sobre MultiTask learning de Richard Socher. Alcanzamos a ver la primera mitad. Lo paramos varias veces para procesarlo. La idea es usar QA como una tarea global donde se pueden instanciar muchas tareas de NLP (e.g., translation, entailment, sentiment analysis). M\u00e1s info en http://decanlp.com/ . Quedamos en retomar el video m\u00e1s adelante y leer el paper con m\u00e1s profundidad. 07/08/2019: Pablo Badilla present\u00f3 su propuesta de Tesis de Mag\u00edster sobre bias en Word Embeddings.","title":"Seminars"},{"location":"seminars/#minutas-in-spanish","text":"25/09/2019: Wladmir Cardoso Brand\u00e3o present\u00f3 InferSent , una t\u00e9cnica para entrenar sentence embeddings usando datos de la Natural Language Inference task. 11/09/2019: Henry Rosales present\u00f3 su art\u00edculo publicado en EMNLP sobre Entity Linking. 04/09/2019: vimos la segunda parte del video de MultiTask Learning. Algunos conceptos interesantes: pointer networks (capas basadas en atenci\u00f3n para copiar partes del input), anti-curriculum training (aprender primero lo m\u00e1s d\u00edficil para evitar quedar en \u00f3ptimo local). 28/08/2019: Jorge di\u00f3 una clase sobre XLNet . Para llegar a XLNet hizo un repaso sobre Attention, Transformer y BERT . Cosas destacables sobre XLNet: relative positional encoding y permutation language models. Un blog post que trata de digerir esto aqu\u00ed . 21/08/2019: Daniel Aguirre present\u00f3 su charla de tesis I de mag\u00edster sobre Transformers para resolver tareas algor\u00edtmicas. 14/08/2019: Vimos este video sobre MultiTask learning de Richard Socher. Alcanzamos a ver la primera mitad. Lo paramos varias veces para procesarlo. La idea es usar QA como una tarea global donde se pueden instanciar muchas tareas de NLP (e.g., translation, entailment, sentiment analysis). M\u00e1s info en http://decanlp.com/ . Quedamos en retomar el video m\u00e1s adelante y leer el paper con m\u00e1s profundidad. 07/08/2019: Pablo Badilla present\u00f3 su propuesta de Tesis de Mag\u00edster sobre bias en Word Embeddings.","title":"Minutas (In Spanish)"}]}