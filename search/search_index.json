{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ReLeLA The REpresentation LEarning and Natural LAnguage Processing (ReLeLa) research group located within the Department of Computer Science (DCC) at the University of Chile focuses on the intersection of representation learning and natural language processing. Members Academic Staff Jorge P\u00e9rez Felipe Bravo-Marquez Claudio Guti\u00e9rrez B\u00e1rbara Poblete Students Constanza Fierro Juan Andr\u00e9s Moreno V\u00edctor Caro Pablo Badilla Juglar D\u00edaz Sebasti\u00e1n Donoso Frank Zamora Jhonny Cerezo Vicente Oyanedel Jos\u00e9 Ca\u00f1ete Daniel Aguirre Cristian Tamblay Henry Rosales Juan-Pablo Silva Camilo Garrido Cristi\u00e1n Ahumada Collaborators Jocelyn Durstan Mauricio Quezada","title":"Home"},{"location":"#relela","text":"The REpresentation LEarning and Natural LAnguage Processing (ReLeLa) research group located within the Department of Computer Science (DCC) at the University of Chile focuses on the intersection of representation learning and natural language processing.","title":"ReLeLA"},{"location":"#members","text":"","title":"Members"},{"location":"#academic-staff","text":"Jorge P\u00e9rez Felipe Bravo-Marquez Claudio Guti\u00e9rrez B\u00e1rbara Poblete","title":"Academic Staff"},{"location":"#students","text":"Constanza Fierro Juan Andr\u00e9s Moreno V\u00edctor Caro Pablo Badilla Juglar D\u00edaz Sebasti\u00e1n Donoso Frank Zamora Jhonny Cerezo Vicente Oyanedel Jos\u00e9 Ca\u00f1ete Daniel Aguirre Cristian Tamblay Henry Rosales Juan-Pablo Silva Camilo Garrido Cristi\u00e1n Ahumada","title":"Students"},{"location":"#collaborators","text":"Jocelyn Durstan Mauricio Quezada","title":"Collaborators"},{"location":"courses/","text":"We teach the following courses: Deep Learning . Natural Language Processing","title":"Courses"},{"location":"pointers/","text":"Pointers Faculty and Evolution of Language The faculty of language: what is it, who has it, and how did it evolve? The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky) http://downs-andrew.blogspot.com/2014/11/a-summary-of-chomsky.html?m=1 Chomsky and Piraha Language http://www.3ammagazine.com/3am/tom-wolfes-reflections-language/ https://www.lavocedinewyork.com/en/2016/10/04/chomsky-we-are-not-apes-our-language-faculty-is-innate/ Statistical (or neural) Language Models Noam Chomsky makes a strong argument about the limitations of statistical language models to model grammar (Syntatic Structures): It is natural to understand \"possible\" as meaning \"highly probable\" and to assume that the linguist's sharp distinction between grammatical and ungrammatical' is motivated by a feeling that since the `reality' of language is too complex to be described completely, he must content himself with a schematized version replacing \"zero probability, and all extremely low probabilities, by impossible, and all higher probabilities by possible.\" We see, however, that this idea is quite incorrect, and that a structural analysis cannot he understood as a schematic summary developed by sharpening the blurred edges in the full statistical picture. If we rank the sequences of a given length in order of statistical approximation to English, we will find both grammatical and ungrammatical sequences scattered throughout the list; there appears to be no particular relation between order of approximation and grammaticalness. Despite the undeniable interest and importance of semantic and statistical studies of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances. I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular https://archive.org/details/NoamChomskySyntcaticStructures/page/n31 There are strong counter-arguments to this: http://norvig.com/chomsky.html https://nlp.stanford.edu/manning/papers/probsyntax.pdf An empirical counter-argument would be the sentences generated by modern deep neural language models: https://openai.com/blog/better-language-models/#fn2 AI http://www.incompleteideas.net/IncIdeas/BitterLesson.html","title":"Pointers"},{"location":"pointers/#pointers","text":"","title":"Pointers"},{"location":"pointers/#faculty-and-evolution-of-language","text":"The faculty of language: what is it, who has it, and how did it evolve? The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky) http://downs-andrew.blogspot.com/2014/11/a-summary-of-chomsky.html?m=1","title":"Faculty and Evolution of Language"},{"location":"pointers/#chomsky-and-piraha-language","text":"http://www.3ammagazine.com/3am/tom-wolfes-reflections-language/ https://www.lavocedinewyork.com/en/2016/10/04/chomsky-we-are-not-apes-our-language-faculty-is-innate/","title":"Chomsky and Piraha Language"},{"location":"pointers/#statistical-or-neural-language-models","text":"Noam Chomsky makes a strong argument about the limitations of statistical language models to model grammar (Syntatic Structures): It is natural to understand \"possible\" as meaning \"highly probable\" and to assume that the linguist's sharp distinction between grammatical and ungrammatical' is motivated by a feeling that since the `reality' of language is too complex to be described completely, he must content himself with a schematized version replacing \"zero probability, and all extremely low probabilities, by impossible, and all higher probabilities by possible.\" We see, however, that this idea is quite incorrect, and that a structural analysis cannot he understood as a schematic summary developed by sharpening the blurred edges in the full statistical picture. If we rank the sequences of a given length in order of statistical approximation to English, we will find both grammatical and ungrammatical sequences scattered throughout the list; there appears to be no particular relation between order of approximation and grammaticalness. Despite the undeniable interest and importance of semantic and statistical studies of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances. I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular https://archive.org/details/NoamChomskySyntcaticStructures/page/n31 There are strong counter-arguments to this: http://norvig.com/chomsky.html https://nlp.stanford.edu/manning/papers/probsyntax.pdf An empirical counter-argument would be the sentences generated by modern deep neural language models: https://openai.com/blog/better-language-models/#fn2","title":"Statistical (or neural) Language Models"},{"location":"pointers/#ai","text":"http://www.incompleteideas.net/IncIdeas/BitterLesson.html","title":"AI"},{"location":"projects/","text":"Spanish Word Embeddings AffectiveTweets","title":"Projects"},{"location":"publications/","text":"2018 Jorge P\u00e9rez, Javier Marinkovi\u0107 and Pablo Barcel\u00f3, On the Turing Completeness of Modern Neural Network Architectures, ICLR 2019. ( pdf ) ( poster ) Aym\u00e9 Arango, Jorge P\u00e9rez, Barbara Poblete , Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation, SIGIR 2019. ( pdf ) Pablo Barcel\u00f3, Nelson Higuera, Jorge P\u00e9rez and Bernardo Subercaseaux, Expressiveness of Matrix and Tensor Query Languages in terms of ML Operators, DEEM @ SIGMOD 2019. ( pdf ) ( slides ) F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research 20(92): Pages 1\u22126, 2019. ( pdf ) S. Lang, F. Bravo-Marquez, C. Beckham, M. Hall, and E. Frank WekaDeeplearning4j: a Deep Learning Package for Weka based on DeepLearning4j , In Knowledge-Based Systems , Volume 178, 15 August 2019, Pages 48-50. DOI: 10.1016/j.knosys.2019.04.013 ( pdf ) A. Ansell, F. Bravo-Marquez, and B. Pfahringer An ELMo-inspired approach to SemDeep-5's Word-in-Context task . In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5) co-located with IJCAI 2019 in Macau, China. ( pdf ) D. Trye, A. S. Calude, F. Bravo-Marquez, and T. T Keegan M\u0101ori Loanwords: A Corpus of New Zealand English Tweets . In Proceedings of the 2019 ACL Student Research Workshop (SRW), Florence, Italy. ( pdf )","title":"Publications"},{"location":"publications/#2018","text":"Jorge P\u00e9rez, Javier Marinkovi\u0107 and Pablo Barcel\u00f3, On the Turing Completeness of Modern Neural Network Architectures, ICLR 2019. ( pdf ) ( poster ) Aym\u00e9 Arango, Jorge P\u00e9rez, Barbara Poblete , Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation, SIGIR 2019. ( pdf ) Pablo Barcel\u00f3, Nelson Higuera, Jorge P\u00e9rez and Bernardo Subercaseaux, Expressiveness of Matrix and Tensor Query Languages in terms of ML Operators, DEEM @ SIGMOD 2019. ( pdf ) ( slides ) F. Bravo-Marquez, E. Frank, B. Pfahringer, and S. M. Mohammad AffectiveTweets: a WEKA Package for Analyzing Affect in Tweets , In Journal of Machine Learning Research 20(92): Pages 1\u22126, 2019. ( pdf ) S. Lang, F. Bravo-Marquez, C. Beckham, M. Hall, and E. Frank WekaDeeplearning4j: a Deep Learning Package for Weka based on DeepLearning4j , In Knowledge-Based Systems , Volume 178, 15 August 2019, Pages 48-50. DOI: 10.1016/j.knosys.2019.04.013 ( pdf ) A. Ansell, F. Bravo-Marquez, and B. Pfahringer An ELMo-inspired approach to SemDeep-5's Word-in-Context task . In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5) co-located with IJCAI 2019 in Macau, China. ( pdf ) D. Trye, A. S. Calude, F. Bravo-Marquez, and T. T Keegan M\u0101ori Loanwords: A Corpus of New Zealand English Tweets . In Proceedings of the 2019 ACL Student Research Workshop (SRW), Florence, Italy. ( pdf )","title":"2018"},{"location":"seminars/","text":"We hold weekly meetings on Wednesdays at 2PM. Our meeting's calendar Minutas (In Spanish) 21/08/08: Daniel Aguirre present\u00f3 su charla de tesis I de mag\u00edster sobre Transformers para resolver tareas algor\u00edtmicas. 14/08/2019: Vimos este video sobre MultiTask learning de Richard Socher. Alcanzamos a ver la primera mitad. Lo paramos varias veces para procesarlo. La idea es usar QA como una tarea global donde se pueden instanciar muchas tareas de NLP (e.g., translation, entailment, sentiment analysis). M\u00e1s info en http://decanlp.com/ . Quedamos en retomar el video m\u00e1s adelante y leer el paper con m\u00e1s profundidad. 07/08/2019: Pablo Badilla present\u00f3 su propuesta de Tesis de Mag\u00edster sobre bias en Word Embeddings.","title":"Seminars"},{"location":"seminars/#minutas-in-spanish","text":"21/08/08: Daniel Aguirre present\u00f3 su charla de tesis I de mag\u00edster sobre Transformers para resolver tareas algor\u00edtmicas. 14/08/2019: Vimos este video sobre MultiTask learning de Richard Socher. Alcanzamos a ver la primera mitad. Lo paramos varias veces para procesarlo. La idea es usar QA como una tarea global donde se pueden instanciar muchas tareas de NLP (e.g., translation, entailment, sentiment analysis). M\u00e1s info en http://decanlp.com/ . Quedamos en retomar el video m\u00e1s adelante y leer el paper con m\u00e1s profundidad. 07/08/2019: Pablo Badilla present\u00f3 su propuesta de Tesis de Mag\u00edster sobre bias en Word Embeddings.","title":"Minutas (In Spanish)"}]}